{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Full Correlational Matrix Analysis (FCMA) demo "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Purpose of this notebook\n",
    "- Given an experimental deisgn that consists two conditions (A and B), we can ask whether the functional connectivities between brain regions differ across conditions A and B. Traditional FC analyses will leverage a seed based FC analysis, which may be biased (seeds are aribtrary selected) and miss potential important regions (Wang et al., 2015). In order to perform whole-brain FC analyses in a non-seed based manner, FCMA was developed. \n",
    "\n",
    "- Using FCMA, we can answer 1) Do functional connectivity patterns differ across condition A and B and 2) what are the brain regions among which the FC pattern defines a task condition. \n",
    "\n",
    "- This jupyter notebook uses simulated data to demo how the above two questions could be answered with FCMA.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Important features of simualted data\n",
    "- Knowing these will help understand FCMA preprocessing step of the features selection and classification. \n",
    "\n",
    "\n",
    "- The simulated data consists of 8 subjects. The experiment was **block designed**, and consists of condition A and B. **Each condition has 10 blocks, thus 20 blocks** in total for a subject. The orders of the blocks were randomized across the 8 subjects. Each **block (i.e., epoch) lasts 15 seconds** followed by **5 seconds of ISI**. The total exepriment lasts 400 seconds in total. **TR = 1 second**. \n",
    "\n",
    "\n",
    "- To reduce computational demands, we randomly selected 400 voxels as the brain mask. Thus the simulated data only has **400 brain voxels**, each voxel has 400 TRs.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from brainiak.fcma.preprocessing import prepare_fcma_data\n",
    "from brainiak.fcma.preprocessing import RandomType\n",
    "from brainiak.fcma.classifier import Classifier\n",
    "from brainiak import io\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import KFold\n",
    "import sys\n",
    "from mpi4py import MPI\n",
    "import logging\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nibabel as nib\n",
    "import os, glob, subprocess\n",
    "from itertools import product\n",
    "import seaborn as sns\n",
    "from nilearn import image, plotting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FCMA Step1: Feature (voxel) selection\n",
    "### The goal: \n",
    "- Given the specified \"left_out_subj\", we devided the 8-subjects data into a training set (7 subjects) and a testing set (1 subject). This is refered as the \"outer loop leave-one-out cross validation\". The goal here is to perform feature selection on the traning set and use selected features to build a predicted model and tested using the testing set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load in the volumes, mask and labels\n",
      "Parse the epoch data for useful dimensions\n",
      "Preprocess the data and prepare for FCMA\n",
      "enforce left one out\n",
      "Take out the idxs corresponding to all participants but this one\n"
     ]
    }
   ],
   "source": [
    "# --------------------\n",
    "# Set up directories\n",
    "# --------------------\n",
    "\n",
    "cur_dir = os.getcwd()\n",
    "data_dir = os.path.join(cur_dir, 'simulated_data')  # What is the directory containing data?\n",
    "suffix = '.nii.gz'  # What is the extension of the data you're loading\n",
    "mask_file = os.path.join(cur_dir, 'cherry_picked_brain',  'cherry_pick_brain_mask.nii.gz')  # What is the path to the whole brain mask\n",
    "epoch_file = os.path.join(cur_dir, 'simulated_data', 'sim_epoch_file.npy')  # What is the path to the epoch file\n",
    "left_out_subj = 0  # Which participant (as an integer) are you leaving out for this cv, for the current demo, set it to 0\n",
    "output_dir = os.path.join(cur_dir, 'FCMA_result')  # What is the path to the folder you want to save this data in\n",
    "\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "    \n",
    "# ----------------------------\n",
    "# load and Preprocess the data\n",
    "# ----------------------------\n",
    "\n",
    "print(\"Load in the volumes, mask and labels\")\n",
    "images = io.load_images_from_dir(data_dir, suffix=suffix)\n",
    "mask = io.load_boolean_mask(mask_file)\n",
    "epoch_list = io.load_labels(epoch_file)\n",
    "\n",
    "print(\"Parse the epoch data for useful dimensions\")\n",
    "epochs_per_subj = epoch_list[0].shape[1]\n",
    "num_subjs = len(epoch_list)\n",
    "\n",
    "print(\"Preprocess the data and prepare for FCMA\")\n",
    "raw_data, _, labels = prepare_fcma_data(images, epoch_list, mask)\n",
    "\n",
    "# ----------------------------------\n",
    "# Outer loop - take out traning set \n",
    "# for feature selection\n",
    "# ----------------------------------\n",
    "\n",
    "print(\"enforce left one out\")\n",
    "start_idx = int(int(left_out_subj) * epochs_per_subj)\n",
    "end_idx = int(start_idx + epochs_per_subj)\n",
    "\n",
    "print(\"Take out the idxs corresponding to all participants but this one\")\n",
    "subsampled_idx = list(set(range(len(labels))) - set(range(start_idx, end_idx)))\n",
    "labels_subsampled = [labels[i] for i in subsampled_idx]\n",
    "raw_data_subsampled = [raw_data[i] for i in subsampled_idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understand preprocessed data\n",
    "- Up to this point, we have our tarining set ready, which includes all epochs (i.e., blocks) and lables for all subjects except subject-0. Now let's look at our training set in more details. \n",
    "- Remember that each subject has 20 epochs (i.e., blocks), evenly divided into two conditions. Given that we have 7 subjects' data in our outer-loop traning set, we have in total 7 x 20 = 140 epochs, thus 140 labels. \n",
    "- Each epoch is a two-dimension array of TR by voxel. In the simulated data, each epoch lasts 15 TRs (TR = 1 sec) and the cherry picked brain has in total 400 voxels. Thus each epoch array has the shape of 15 by 400. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label count: 140\n",
      "Epoch (block) count: 140\n",
      "Shape of each epoch (block): (15, 400)\n"
     ]
    }
   ],
   "source": [
    "print(f\"Label count: {len(labels_subsampled)}\")\n",
    "print(f\"Epoch (block) count: {len(raw_data_subsampled)}\")\n",
    "print(f\"Shape of each epoch (block): {raw_data_subsampled[0].shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature selection with simplified implementation\n",
    "- Please note that the following code chunk is an extremely primitive version of FCMA feature selection and is not practical for real data due to computation limits. The actual FCMA codes ultilized the message passing interface (MPI) which requests at least 2 processors for a job. Because jupyter notebook typically only allow for 1 processor, the actualy FCMA code cannot be used for this demo. However, this primitive version is designed to be a conceptual implementation of the feature selection step for FCMA and can be used to understand what exactly happens under the hood\n",
    "\n",
    "#### Steps:\n",
    "\n",
    "1. First, we enlarge the feature space by computing the correltaion matrix of all voxels in the barin. Becuase the simulated brain only has 400 voxels, this yields a 400 x 400 correlation matrix. Each row/column is the functional connectivity between a given voxel and all other voxels in the brain. (Note that this is extemely not efficient computational wise and this was done using the kernel trick (the precomputed kernel) in the acutal FCMA implementation.)\n",
    "\n",
    "2. For each barin voxel, we want to know how well does its functional connectivity with every other brain voxels can be used to characterize the task conditions. Thus, for each brain voxel, we can \n",
    "    - A) Extract the correpsonding row from each subject's correlation matrix, resulting a single array for each subject.  \n",
    "    - B) Perform the inner loop leave-one-out cross validation. For each fold, 7-1 = 6 arrays (one for each subjects) will be used as the training set and the left array will be used as the validation set. \n",
    "    - C) A prediction accuracy value for this selected voxel could be computed for each fold, \n",
    "    - D) Final prediction accuracy for this voxel could be averaged across the 7 folds. \n",
    "\n",
    "    - It seems computationally managable here because the simulated data only has 400 voxels. The actual FCMA code ultilized MPI for parallel computation, so multiple voxels could be dealt with at the same time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# Enlargeing the feature space \n",
    "# -----------------------------\n",
    "raw_data_subsampled_mat = [np.corrcoef(sample.T) for sample in raw_data_subsampled]\n",
    "\n",
    "\n",
    "# ------------------------------------------\n",
    "# Inner loop leave one out cross validation\n",
    "# ------------------------------------------\n",
    "vox_score = []\n",
    "# for every voxel in the brain\n",
    "for i in range(raw_data_subsampled_mat[0].shape[0]):\n",
    "    \n",
    "    # A) extract the corresponding row from each subjects' correlation matrix \n",
    "    raw_data_sub_selected_vox = [sample[i,] for sample in raw_data_subsampled_mat]\n",
    "\n",
    "    # B) Perform the inner loop leave-one-out cross validation\n",
    "    kf = KFold(n_splits = num_subjs - 1)\n",
    "    score = []\n",
    "    for train_ind, test_ind in kf.split(labels_subsampled):\n",
    "\n",
    "        # For each fold, get training and validation set \n",
    "        vox_train, vox_test = [raw_data_sub_selected_vox[ind] for ind in train_ind], [raw_data_sub_selected_vox[ind] for ind in test_ind]\n",
    "        lab_train, lab_test = [labels_subsampled[ind] for ind in train_ind], [labels_subsampled[ind] for ind in test_ind]\n",
    "\n",
    "        # set up the model\n",
    "        svm_clf = SVC(kernel='linear', shrinking=False, C=1)\n",
    "\n",
    "        # train the model\n",
    "        svm_clf.fit(vox_train, lab_train)\n",
    "\n",
    "        # test the model\n",
    "        predict = svm_clf.predict(vox_test)\n",
    "        \n",
    "        # C) A prediction accuracy value for this selected voxel could be computed for each fold\n",
    "        score.append(svm_clf.score(vox_test, lab_test))\n",
    "    \n",
    "    # D) Final prediction accuracy for this voxel\n",
    "    vox_score.append(np.mean(score))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understand the outputs of feature selection\n",
    "- For each inner loop fold, we end up getting a list of prediction accuracy, one for each voxel in the brain. Thus the length of this list equals the total number of voxels in the brain (i.e., 400). \n",
    "- We can then rank these voxels based on their classification accuracies. Thus each voxel, in addition to have a preidction accuracy value, would also have a rank value. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The length of the prediction accuracy list is 400\n",
      "The top ranked 10 voxel IDs are [117, 247, 261, 7, 65, 110, 118, 125, 368, 88]\n"
     ]
    }
   ],
   "source": [
    "# rank all voxels based on their prediction accuracies, the higher the acc, the higher the rank \n",
    "vox_dic = {vid:score for vid, score in zip(range(len(vox_score)), vox_score)}\n",
    "vox_rank = sorted(vox_dic, key=vox_dic.get, reverse=True)\n",
    "\n",
    "print(f\"The length of the prediction accuracy list is {len(vox_score)}\")\n",
    "print(f\"The top ranked 10 voxel IDs are {vox_rank[0:10]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform feature selection with actual FCMA implementation\n",
    "- After understanding what the feature selection step does, lets now run the actual FCMA feature selection code. This actual code will go through everything we have done up to this point, but 1) with a much finer and efficient implementation and 2) repeating it 8 times, with each time leaving out a different subject (the example code only is only one fold of the outer loop, leaving out subject 0). \n",
    "- If you run this locally, expect it to take a couple minutes. You can monitor your terminal for progress. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use subprocess to run a python script with MPI. This should work on a cluster or local computer. \n",
    "for left_sid in range(num_subjs):\n",
    "    subprocess.run(f\"mpirun -np 2 python3 ./FCMA_script/fcma_voxel_selection_cv.py {data_dir} {suffix} {mask_file} {epoch_file} {left_sid} {output_dir}\", shell = True) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Now lets look at its outputs, and lets focus on those from the first fold that left out subject0. \n",
    "    - The '*.txt' is basically our 'vox_dic' in the demo code, but in a ranked manner. \n",
    "    - Each brain voxel in the '*score.nii.gz' file was assigned with its predcition accuracy. \n",
    "    - Each brain voxel in the '*seq.nii.gz' file was assigned with its prediction accuracy rank (rank = 0 means the highest acc). Using this file, we could select the top performed brain voxels to form a 'top_n_mask', with n being the number of the top-performed voxels selected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/Users/peetal/Desktop/FCMA_demo/FCMA_result/fc_no0_result_seq.nii.gz',\n",
       " '/Users/peetal/Desktop/FCMA_demo/FCMA_result/fc_no0_result_score.nii.gz',\n",
       " '/Users/peetal/Desktop/FCMA_demo/FCMA_result/fc_no0_result_list.txt']"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glob.glob(os.path.join(output_dir, 'fc_no0*'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The top ranked 10 voxel IDs from the FCMA output. Notice that they are almost exactly the same as those identified using the primitive code above. The differences in numeric value could be due to the much finer implementation of FCMA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['7', '88', '117', '157', '261', '118', '65', '125', '368', '247']"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(output_dir + '/fc_no0_result_list.txt', 'r') as f:\n",
    "    line = f.readlines()\n",
    "[l.split(' ')[0] for l in line[0:10]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select n top performed features\n",
    "- Remember that so far we have got 8 '*seq.nii.gz' files, one for each left out subjects. Now we are going to select the N top-performed voxels to form brain masks for each left out subject. \n",
    "- The value N could be arbitrarily decided. Here we show N = 10, 20, 30. Small numbers were used to reduce computation demands. \n",
    "- If you are running this on a cluster, you need to un-comment the \"module load fsl\" command to load fsl first. If it returns an error (which should not happen because we are using subprocess), you may want to write **\"module load fsl/6.0.1\"** in a **.bashrc-ondemand** file in your home directory. So that fsl could be load once you start jupyter notebook.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make top n masks\n",
    "for n in [10,20,30]:\n",
    "    top_n = 'top_' + str(n)\n",
    "    #subprocess.run(\"module load fsl\", shell = True)\n",
    "    subprocess.run(f\"bash ./FCMA_script/make_top_voxel_mask.sh {output_dir} {n} {os.path.join(output_dir, top_n)}\", shell = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Classification using the selected features (voxels)\n",
    "\n",
    "#### The goal\n",
    "- Now we have selected top N features (voxels) from the original brain mask. Now we ask if we only account for the connectivity pattern within these selected features (voxels), how well can a classifier do in terms of differentiating two conditions? This is so called intrinsic classification. The rationale is that if a classifier trained on the FC pattern of the selected features is enough to differentiate two task conditions, then we could say that the selected features are the brain regions among which the FC changes characterize a certain task condition. \n",
    "\n",
    "- For the demo, we will still focus on Subject 0. During the feature selection step, we have selected top N features (voxels) using the traning set (the data of the rest 7 subjects). Now we are going to use these features to build a predictive model and use subject 0 as a testing data to finish the first fold of our outer loop cross validation. \n",
    "\n",
    "#### Steps\n",
    "\n",
    "1. Preprocess all 8 subjects' data as we have discussed above, yielding a list of labels of the length equals 20 (epochs/subject) * 8 (subjects) = 160, and a list of epoch (i.e., block) data of the same length. Within each epoch data (i.e., each entry in the variable \"int_data\" in the code chunk below), is a 2d array of [TR, nVoxel]. Note that nVoxel depends on the size of the top n masks we are using.\n",
    "\n",
    "2. Divide the data into training and testing set. This is exactly the same as the feature selection step except that now we have already selected meaningful features from the traning set. In this demo, the trianing set is subject 1 - 7 and the testing set is subject 0. \n",
    "\n",
    "3. Train a model using the FC pattern of selected features (intrinsic classification). \n",
    "\n",
    "4. Test the model using the left out subject. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--TOP 10 Voxels RESULTS--\n",
      "Top feature size: TOP 10 Voxels\n",
      "[-4.20675165 -3.88490625  0.56542723 -1.00673355  0.44397475  0.98067244\n",
      " -2.67554682 -4.59680603 -5.10849732  1.31384181  1.12661917 -2.14709121\n",
      "  0.71594991  2.44972832  3.38266405  5.47356223  2.24419188  2.06794301\n",
      "  0.84560292  1.12130425]\n",
      "[0 0 1 0 1 1 0 0 0 1 1 0 1 1 1 1 1 1 1 1]\n",
      "When leaving subject 0 out for testing using the /Users/peetal/Desktop/FCMA_demo/FCMA_result/top_10/fc_no0_result_seq_top10.nii.gz mask for an intrinsic correlation, the accuracy is 15 / 20 = 0.75\n",
      "---------------------------------------------------------------------------------------------------\n",
      "--TOP 20 Voxels RESULTS--\n",
      "Top feature size: TOP 20 Voxels\n",
      "[-0.48627324 -0.84935391 -0.27051834 -0.34932846 -1.46580457 -0.44201257\n",
      " -0.83391803 -2.25831648 -0.52109797 -0.44800317  0.37804332  0.07782005\n",
      "  0.91908168  0.63763344  1.45488306  1.54257089  1.0691264   1.13135145\n",
      "  0.80214601  0.6720513 ]\n",
      "[0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1]\n",
      "When leaving subject 0 out for testing using the /Users/peetal/Desktop/FCMA_demo/FCMA_result/top_20/fc_no0_result_seq_top20.nii.gz mask for an intrinsic correlation, the accuracy is 20 / 20 = 1.00\n",
      "---------------------------------------------------------------------------------------------------\n",
      "--TOP 30 Voxels RESULTS--\n",
      "Top feature size: TOP 30 Voxels\n",
      "[-0.28538792 -0.0522221  -0.36699865 -0.25605295 -0.68587767 -0.34833759\n",
      " -0.69794263 -1.08978284 -0.44282535 -0.38892418  0.3537948  -0.60454384\n",
      "  0.65464748  0.49777431  0.4322301   1.02326911  0.7271251   0.73474396\n",
      "  0.51744501  0.55475626]\n",
      "[0 0 0 0 0 0 0 0 0 0 1 0 1 1 1 1 1 1 1 1]\n",
      "When leaving subject 0 out for testing using the /Users/peetal/Desktop/FCMA_demo/FCMA_result/top_30/fc_no0_result_seq_top30.nii.gz mask for an intrinsic correlation, the accuracy is 19 / 20 = 0.95\n",
      "---------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------------------\n",
    "# top n feature mask from the training set \n",
    "# ------------------------------------------\n",
    "# path to 3 top feature masks \n",
    "top_n_mask_files = {os.path.join(output_dir, \"top_10/fc_no0_result_seq_top10.nii.gz\") : 'TOP 10 Voxels',\n",
    "                    os.path.join(output_dir, \"top_20/fc_no0_result_seq_top20.nii.gz\") : 'TOP 20 Voxels',\n",
    "                    os.path.join(output_dir, \"top_30/fc_no0_result_seq_top30.nii.gz\") : 'TOP 30 Voxels'}\n",
    "\n",
    "# for each top feature mask: \n",
    "for top_n_mask_path in top_n_mask_files.keys():\n",
    "    \n",
    "    # ----------------\n",
    "    # preprocess data \n",
    "    # ----------------\n",
    "    # Load in the volumes, mask and labels\n",
    "    images = io.load_images_from_dir(data_dir, suffix=suffix)\n",
    "    top_n_mask = io.load_boolean_mask(top_n_mask_path)\n",
    "    epoch_list = io.load_labels(epoch_file)\n",
    "\n",
    "    # Parse the epoch data for useful dimensions\n",
    "    epochs_per_subj = epochs_per_subj = epoch_list[0].shape[1]\n",
    "    num_subjs = len(epoch_list)\n",
    "\n",
    "    # Prepare the data\n",
    "    int_data, _, labels = prepare_fcma_data(images, epoch_list, top_n_mask)\n",
    "\n",
    "    # -------------------\n",
    "    # Outer loop testing\n",
    "    # -------------------\n",
    "\n",
    "    # What indexes pick out the left out participant?\n",
    "    start_idx = int(int(left_out_subj) * epochs_per_subj)\n",
    "    end_idx = int(start_idx + epochs_per_subj)\n",
    "\n",
    "    # Take out the idxs corresponding to all participants but this one\n",
    "    training_idx = list(set(range(len(labels))) - set(range(start_idx, end_idx)))\n",
    "    testing_idx = list(range(start_idx, end_idx))\n",
    "\n",
    "    # Pull out the data\n",
    "    int_data_training = [int_data[i] for i in training_idx]\n",
    "    int_data_testing = [int_data[i] for i in testing_idx]\n",
    "\n",
    "    # Pull out the labels\n",
    "    labels_training = [labels[i] for i in training_idx]\n",
    "    labels_testing = [labels[i] for i in testing_idx]\n",
    "\n",
    "    # Prepare the data to be processed efficiently (albeit in a less easy to follow way)\n",
    "    rearranged_int_data = int_data_training + int_data_testing\n",
    "    rearranged_labels = labels_training + labels_testing\n",
    "    num_training_samples = epochs_per_subj * (num_subjs - 1)\n",
    "\n",
    "    # Set up data so that the internal mask is correlated with the internal mask\n",
    "    corr_obj = list(zip(rearranged_int_data, rearranged_int_data))\n",
    "\n",
    "    # no shrinking, set C=1\n",
    "    svm_clf = SVC(kernel='precomputed', shrinking=False, C=1)\n",
    "\n",
    "    clf = Classifier(svm_clf, epochs_per_subj=epochs_per_subj)\n",
    "\n",
    "    # Train the model on the training data\n",
    "    clf.fit(corr_obj, rearranged_labels, num_training_samples)\n",
    "\n",
    "    # Test on the testing data\n",
    "    predict = clf.predict()\n",
    "\n",
    "    # ---------------------\n",
    "    # Print out the results\n",
    "    # ---------------------\n",
    "    \n",
    "    print(f'--{top_n_mask_files[top_n_mask_path]} RESULTS--')\n",
    "    print(f\"Top feature size: {top_n_mask_files[top_n_mask_path]}\")\n",
    "    print(clf.decision_function())\n",
    "    print(clf.predict())\n",
    "\n",
    "    # How often does the prediction match the target\n",
    "    num_correct = (np.asanyarray(predict) == np.asanyarray(labels_testing)).sum()\n",
    "\n",
    "\n",
    "    # Report accuracy\n",
    "    print( 'When leaving subject %d out for testing using the %s mask for an intrinsic correlation, the accuracy is %d / %d = %.2f' %\n",
    "        (int(left_out_subj), top_n_mask_path,  num_correct, epochs_per_subj, num_correct / epochs_per_subj))\n",
    "    print('---------------------------------------------------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understand the outputs of classification\n",
    "1. **Decision function outputs**: a list of floats that has the length equals the number of epochs (in the case of a two way classification). Each float indicates an epoch's location relative to the hyperplane defined in SVM. The sign of the output indicates the class of the epoch and the absolute value indicates the confidence of the classification of this epoch. \n",
    "2. **Class prediction**: a list of ints that has the length equals the number of epochs (in the case of a two way classification). Each int indicates the classified class of the epoch. \n",
    "3. **Classification accuracy**: correctly classified epoch number / total epoch number\n",
    "\n",
    "### Perform classification for all outer loop folds using FCMA script\n",
    "- The outer loop has 8 folds (8 subject) and each subject has 3 level of top n mask. Thus in total 24 classifications would be performed. \n",
    "- The whole process should take around 10 minutes depends on your laptop. You do not have to run this as the output is already included in the folder. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Left out subject 0, Top 10 voxel mask, Classification done\n",
      "Left out subject 1, Top 10 voxel mask, Classification done\n",
      "Left out subject 2, Top 10 voxel mask, Classification done\n",
      "Left out subject 3, Top 10 voxel mask, Classification done\n",
      "Left out subject 4, Top 10 voxel mask, Classification done\n",
      "Left out subject 5, Top 10 voxel mask, Classification done\n",
      "Left out subject 6, Top 10 voxel mask, Classification done\n",
      "Left out subject 7, Top 10 voxel mask, Classification done\n",
      "Left out subject 0, Top 20 voxel mask, Classification done\n",
      "Left out subject 1, Top 20 voxel mask, Classification done\n",
      "Left out subject 2, Top 20 voxel mask, Classification done\n",
      "Left out subject 3, Top 20 voxel mask, Classification done\n",
      "Left out subject 4, Top 20 voxel mask, Classification done\n",
      "Left out subject 5, Top 20 voxel mask, Classification done\n",
      "Left out subject 6, Top 20 voxel mask, Classification done\n",
      "Left out subject 7, Top 20 voxel mask, Classification done\n",
      "Left out subject 0, Top 30 voxel mask, Classification done\n",
      "Left out subject 1, Top 30 voxel mask, Classification done\n",
      "Left out subject 2, Top 30 voxel mask, Classification done\n",
      "Left out subject 3, Top 30 voxel mask, Classification done\n",
      "Left out subject 4, Top 30 voxel mask, Classification done\n",
      "Left out subject 5, Top 30 voxel mask, Classification done\n",
      "Left out subject 6, Top 30 voxel mask, Classification done\n",
      "Left out subject 7, Top 30 voxel mask, Classification done\n"
     ]
    }
   ],
   "source": [
    "# run classification for each left out subject for each top-n-mask size\n",
    "for n, left_sid in product([10,20,30],range(8)):\n",
    "    \n",
    "    top_n_mask_file_path = os.path.join(output_dir, f'top_{n}', f'fc_no{left_sid}_result_seq_top{n}.nii.gz')\n",
    "    results_path = os.path.join(output_dir, f'top_{n}')\n",
    "    \n",
    "    subprocess.run(f\"mpirun -np 2 python3 ./FCMA_script/fcma_classify.py {data_dir} {suffix} {top_n_mask_file_path} {epoch_file} {left_sid} {results_path}\", shell = True) \n",
    "    \n",
    "    print(f\"Left out subject {left_sid}, Top {n} voxel mask, Classification done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Examine classification accuracy for different top-n-mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYkAAAEICAYAAACqMQjAAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3deXxU9b3/8deHQICEHQIqYRFEkaqghlBrW7eq2F5/FmvdKipLkV7ttbXa2vbe7ra21i7+9F5KBUFaxaWlolKXaq9bEQIKsggawxZBIOwEQkjyuX/MSRxCDkzCHCaTeT8fDx4zZ5t84jF55/udOZ9j7o6IiEhDWqW6ABERab4UEiIiEkohISIioRQSIiISSiEhIiKhFBIiIhKqdaoLSKYePXp4//79U12GiEhaWbhwYZm75zW0rUWFRP/+/VmwYEGqyxARSStmtiZsm6abREQklEJCRERCKSRERCSUQkJEREIpJEREJJRCQkREQikkREQkVIu6TkJEJNlGT5lH6ba95Hdtz4xxI1JdzlEX+UjCzEaa2UozKzazOxvY3tXMZpnZO2Y238xOidu22syWmNkiM9NVciJy1JVu28uqsnJKt+1NdSkpEelIwsyygAeAC4FSoMjMZrv78rjdvgcscvdRZjY42P+CuO3nuXtZlHWKiEjDoh5JFALF7l7i7pXATOCyevsMAV4CcPcVQH8z6xVxXSIikoCoQ6I3sC5uuTRYF28xcDmAmRUC/YD8YJsDL5jZQjObEHGtIiJST9RvXFsD67ze8t3A781sEbAEeBuoCrad7e7rzawn8KKZrXD3Vw/4ArHwmADQt2/fpBYvIpLpoh5JlAJ94pbzgfXxO7j7Tncf4+7DgOuBPGBVsG198LgJmEVs+op6x0929wJ3L8jLa7DTrYiINFHUIVEEDDKz480sG7gamB2/g5l1CbYBjAdedfedZpZrZh2DfXKBi4ClEdcrIiJxIp1ucvcqM7sFeB7IAqa6+zIzmxhsnwScDDxsZtXAcmBccHgvYJaZ1db5iLs/F2W9IiJyoMgvpnP3OcCceusmxT2fCwxq4LgSYGjU9YkcDZl+QZakL11xLXIU1F6QJZJu1LtJRERCKSRERCSUQkJEREIpJEREJJRCQkREQikkREQklEJCRERCKSRERCSUQkJEREIpJEREJJRCQkREQikkREQklEJCRERCKSRERCSUQkJEREIpJEREJJRCQkREQikkREQklEJCRERCKSRERA7B3Q94zDQKCRGRBmzfU8kPnlrKmq17AFizdQ8/fnoZOyv2p7iyo6t1qgsQEWludlXs58o/zOW9jbvr1rnDQ2+sZv6qrTwx8SxysjPj16dGEiIi9Ux7Y/UBARFv2fqdPDJv7VGuKHUUEiIiQFV1DavLynl5xUamz119yH2fWrT+qNTUHGTGeElEJLBj735KNu+mZHM5H8Q9rtmyh8rqmoRfI1NEHhJmNhL4PZAFPOjud9fb3hWYCgwEKoCx7r40kWNFRBpSXeOUbttTFwAfxAVC2e59R/z6J/bqmIQq00OkIWFmWcADwIVAKVBkZrPdfXncbt8DFrn7KDMbHOx/QYLHikgG21mxPxYEm3ZTUrabDzaVU1K2m9VliY8KAPI6tmVAj1wG9uzAgB657Kyo4r6X3g/d/4ZP9UtG+Wkh6pFEIVDs7iUAZjYTuAyI/0U/BPgFgLuvMLP+ZtYLGJDAsSLSwlXXOB9u2xuMCGKjgpLgsTGjguysVvTvkcPAvA4MyMsNHmPPO7Vrc9D+NTXO/f8sPmj97RedyGcG5R3R95ROog6J3sC6uOVSYES9fRYDlwOvm1kh0A/IT/BYEWkhakcFJUEYxJ6Xs2pLOZVViY8KenRoy8C8XAbkdWBgXRjkkt81h6xWlvDr3H7xSVz8iWO49sE32VVRRad2rZk54SyGHNepKd9e2oo6JBo6I/UvW7wb+L2ZLQKWAG8DVQkei5lNACYA9O3b94iKFZFo1Y0KynYHU0Qfjwo272r8qGBAjw4M7JnLgB6xIBiQ14HO7Q8eFTTVqfmd6dGhLbsqqujeoW3GBQREHxKlQJ+45XzggM+OuftOYAyAmRmwKviXc7hjg+MnA5MBCgoKMvO6eZFmZlftewW1I4Lg/YKmjApiU0O5B0wTNXZUIE0XdUgUAYPM7HjgQ+Bq4Nr4HcysC7DH3SuB8cCr7r7TzA57rIikTnWNs377XorrpoY+niba1MhRQb/uOQe8TzAwglGBNE2kIeHuVWZ2C/A8sY+xTnX3ZWY2Mdg+CTgZeNjMqom9KT3uUMdGWa+IHKx2VFBStrve6KCxo4LsA6aHah/zu7andZau622uIr9Owt3nAHPqrZsU93wuMCjRY0Uk+WpqnA+37z3g4rLax8aMCtpkGf2658a9cRxMEfXoQOccjQrSka64Fskgu/dVNXi18aqycvY1YlTQPTe73kdJa98r0KigpVFIiLQwtaOCkrKDLzLbuLPxo4L4i8wG9uygUUGGUUiIpKnd+6pYVTci+Lj1RFNGBfVHBAPyOtBHowJBISESqSWlO3jw9RLWBjeu2VK+j492VHBM53YJHV9T46zfsTfuKuOPp4gaMypo3cro1z3ngKuMBwafIuqSk92k700yg0JCJCJzlmzg64+8TXXcbS937q3iC/e9xmM3fZITen7cJK58X1Xc9QS7+SCYKlq9pZyK/YmPCrrlZsfeNK53kVmfbjm00ahAmkAhIRKBXRX7uePJxQcERK0t5ZWMnVbEOSf2rBsZfLSzIuHXrh0VHPDpoSAYuuZqVCDJpZAQicCcJRso31cdun3t1r3MeHPNIV+jW2527M3ivI9bTgzUqECOMoWESATWb09sZNC6ldG3e84B1xPUThNpVCDNgUJCJALHdTn8G9PTxgzn7BN6aFQgzZpCQiQCbVtnHXL78P5dOfeknkepGpGm058wIkn2xIJ13Pb4otDt3XOz+cXlpx7FikSaTiMJkSR68LUSfvbsu3XLl5/Rm5oa55l3NlBV43Ru34Zn/+MzCV8nIZJqCgmRJHB37nl+Jf/9vx/UrbvzksFMPGcgAItLd7CqrJxuudkKCEkrCgmRI1Rd4/zn35by6Py1ALQy+PmoU7m6UHdKlPSnkBA5AvuqqrntscU8u2QDELuBzu+vHsYlpx6b4spEkkMhIdJE5fuqmPinhbz2fhkAOdlZTB5dwKcH9UhxZSLJo5AQaYJt5ZWMmVbEonXbAeia04aHxhQyrE+XFFcmklwKCZFG+mhHBaOnzOP9TbsBOLZzO2aMKzygYZ9IS6GQEGmEVWXlXPfgPD7cvheAAT1ymTF+BL27tE9xZSLRUEiIJGjphzu48aH5lO2uBOCU3p2YPqaQ7h3aprgykegoJEQSMK9kC+OnL2DXvioAPjmgG3+8voCO7XQbT2nZFBIih/GP5Ru5+ZG36m4JeuGQXvz/a06nXZtD92cSaQkUEiKH8Ne3SrnjyXeorondPOiKM/O5+/JTde9nyRgKCZEQU19fxU+eWV63PP7Tx/O9z59Mq1aWwqpEji6FhEg97s5vX3yP+14urlt3x8Un8e/nDsRMASGZJfIxs5mNNLOVZlZsZnc2sL2zmT1tZovNbJmZjYnbttrMlpjZIjNbEHWtIjU1zg+eWlYXEBb0Ybr5vBMUEJKRIh1JmFkW8ABwIVAKFJnZbHdfHrfbzcByd7/UzPKAlWb2Z3evDLaf5+5lUdYpAlBZVcO3nljM04vXA9Amy/jdVafzhdPUh0kyV9TTTYVAsbuXAJjZTOAyID4kHOhosT/TOgBbgaqI6xI5wJ7KKr72p7d45b3NALRvk8UfRp/JZ0/MS3FlIqkV9XRTb2Bd3HJpsC7e/cDJwHpgCXCru9cE2xx4wcwWmtmEiGuVDLVjz35GT5lfFxCd27fhz18doYAQIfqRREOTuF5v+WJgEXA+MBB40cxec/edwNnuvt7MegbrV7j7qwd8gVh4TADo21f9+6VxNu2sYPSU+azcuAuAXp3aMmPcCE7spT5MIhD9SKIU6BO3nE9sxBBvDPBXjykGVgGDAdx9ffC4CZhFbPrqAO4+2d0L3L0gL09/+Uni1mwp50uT/lUXEP275/DkxE8pIETiRB0SRcAgMzvezLKBq4HZ9fZZC1wAYGa9gJOAEjPLNbOOwfpc4CJgacT1SoZ4d8NOrpg0l3VbY436hhzbiScmfoo+3XJSXJlI8xLpdJO7V5nZLcDzQBYw1d2XmdnEYPsk4KfANDNbQmx66jvuXmZmA4BZwccOWwOPuPtzUdYrmWHB6q2MmVbErorY5yMK+3fjwRsL6KQ+TCIHifxiOnefA8ypt25S3PP1xEYJ9Y8rAYZGXZ9kln+u2MTX/ryQiv2xz0ZcMLgnD3zlDPVhEgmhK64lYzy16EO+9fhiqoI+TKNO782vrjiNNurDJIeQ37X9AY+ZRiEhGeHhuav54exlePDZujFn9+e/vjBEfZjksGaMG5HqElJKISEtmrtz30vF/PYf79Wtu+3CE/n6+WqzIZIIhYS0WDU1zk+eWc60f60GYn2YfvL/PsHos/qntC6RdKKQkBZpf3UNdzyxmL8til2W07qVce+VQ7lsWP0L/kXkUBQS0uLsrazm5kfe4uUVmwBo16YVk647k3NP6pniykTST8If6zCzUWbWOW65i5l9MZqyRJpmx979XD91Xl1AdGrXmj+PH6GAEGmixnz274fuvqN2wd23Az9MfkkiTbNpVwVX/WEuRau3AZDXsS2P3XQWZ/brluLKRNJXY6abGgoUTVdJs7Bu6x6umzKPNVv2ANC3Ww5/GjeCvt3VZkPkSDTml/wCM/sNsZsIOfB1YGEkVYk0woqPdnL9lPls2rUPgMHHdOThsYX07NQuxZWJpL/GTDd9HagEHgMeB/YSu6ucSMosXLONKyfNrQuIgn5deWzCWQoIkSRJeCTh7uXAQfeoFkmVV97bzMQZC9m7vxqAc0/K43++cibts9WHSSRZGvPpphfNrEvcclczez6askQO7enF6xk/vaguIC4bdhx/vL5AASGSZI15T6JH8IkmANx9W3DHOJGjasaba/jBU0vr+jBdf1Y/fnTpJ9SHSSQCjQmJGjPr6+5rAcysHwffilQkMu7O/S8Xc++LH/dhuvWCQXzjc4PUh0kkIo0Jie8Dr5vZK8HyZwnuLS0StZoa52fPvsvUN1bVrfvhpUMYc/bxKaxKpOVrzBvXz5nZGcAnid1B7pvuXhZZZSKB/dU1fOcv7/DXtz4EIKuVce+Xh/LF09WHSSRqjb0YrhrYBLQDhpgZ7v5q8ssSianYX80tj7zFP96Ntdlo27oV/3PdGZw/uFeKK2ucTL9xjaSvhEPCzMYDtwL5wCJiI4q5wPnRlCaZbmfFfsZPX8D8VVsB6Ni2NVNuHE7h8enXZiPTb1wj6asxF9PdCgwH1rj7ecDpwOZIqpKMV7Z7H9dMfrMuIHp0aMvMmz6ZlgEhks4aM91U4e4VZoaZtXX3FWZ2UmSVScZat3UP10+dz6qyciA2RfOncSPo3yM3xZWJZJ7GhERpcDHd34AXzWwbsD6asiRTvb9xF6OnzOejnRUAnNirAzPGjaCX2myIpERjPt00Knj6IzP7J9AZeK52u5l1dfdtSa5PMsjba7cxZloR2/fsB+D0vl146MbhdMnJTnFlIpmrSa2+3f2VBla/BJxxZOVIpnrt/c3cNGMheypjbTY+M6gHfxh9JjnZ6kYvkkrJ/AnUJa/SJHOWbODWmW+zvzp2Af8XTjuW3145jOzWjflchYhEIZkhoRYd0miPzl/L92YtqevDdO2Ivvz0slPIUh8mkWYh8j/VzGykma00s2IzO6jVuJl1NrOnzWyxmS0zszGJHivpy9357/8t5rt//TggbjnvBO76ogJCpDk5bEiYWaLNcQ76yTazLGJ3srsEGAJcY2ZD6u12M7Dc3YcC5wL3mll2gsdKGnJ3fvH3FfzquZV16/7zCydz+8UnqVGfSDOTyEjiSQAze+kw+13QwLpCoNjdS9y9EpgJXFZvHwc6Wuy3QwdgK1CV4LGSZqqqa/j2k+8w+dUSINaH6ddfHsr4zwxIcWUi0pBE3pNoZWY/BE40s9vqb3T33wSPWxs4tjewLm65FKjfn+B+YDaxay46Ale5e42ZJXIsZjaBoBtt3759E/h2JFUq9lfzH4++zQvLNwKQ3boVD1x7BhcOSa8+TCKZJJGRxNVABbFA6djAv0NpaO6g/hvcFxPrBXUcMAy438w6JXgs7j7Z3QvcvSAvL+8w5Uiq7KrYz5iHiuoCokPb1jw8tlABIdLMHXYk4e4rgV+a2Tvu/vdGvn4p0CduOZ+Dr9IeA9zt7g4Um9kqYHCCx0oa2LJ7Hzc+VMSSD3cA0D03m+ljCzmld+cUVyYih3PYkIifYjKzk+tvr51uClEEDAre/P6Q2Kjk2nr7rCX2fsZrZtYLOAkoAbYncKw0cx9u38voKfMo2Rzrw9S7S3tmjCtkQF6HFFcmIolI5D2J2ikl5+ApoENeG+HuVWZ2C/A8kAVMdfdlZjYx2D4J+CkwzcyWBK//ndqbGTV0bGLfVsszeso8SrftJb9r+7RpO128aTejp8xjw45YH6YTenZgxrhCju2seyqIpItEppt+DGBm04Fb3X17sNwVuDeB4+cAc+qtmxT3fD1wUaLHZqrSbXvruqKmg3dKt3PD1PlsC/owDc3vzENjCumWqz5MIumkMVdcn1YbEADuvs3MTo+gJklz/you46sPL6A86MN09gnd+cPoAjq0VR8mkXTTmCuuWwWjBwDMrBvJbeshLcBzSz/ixoeK6gLiklOOYeqNwxUQImmqMT+59wL/MrMnib0XcSVwVyRVSVp6vGgdd/71HWqCd6quHt6Hu0adqjYbImmsMfeTeNjMFhC7p7UBl7v78sgqk7Qy+dUP+PmcFXXLE88ZyHdGqs2GSLpr1BxAEAoKBqnj7vzyuZVMeuWDunXfvWQwN50zMIVViUiyaKJYmqy6xvn+rCXMLIp1T2llcPflp3Hl8D6HOVJE0oVCQppkX1U135i5iL8v/QiA7KxW3HfN6Yw85ZgUVyYiyaSQkEYr31fFTTMW8npxGQC52Vn88YYCPjWwR4orE5FkU0hIo2wrr+TGaUUsXhe7ZKZbbjbTxgzntPwuKa5MRKKgkJCEbdixl9FT5lO8aTcAx3Vux8PjRnBCT/VhEmmpFBKSkJLNuxk9ZT4fbt8LwIC8XGaMG0HvLurDJNKSKSTksJZ+uIMbps5nS3klAKfld+ahG4fTvUPbFFcmIlFTSMghvVmyhfHTF7B7XxUAZw3ozh9vUB8mkUyhn3QJ9eLyjdz8yFtUVtUAcNGQXtx3zem0a5OV4spE5GhRSEiD/rKwlG//5R2qg0ZMVxbk8/NRp9I6qzE9IUUk3Skk5CAPvlbCz559t255wmcH8N1LBqsPk0gGUkhIHXfn3hfe4/5/Ftet+87IwXztXPVhEslUCgkBYn2YfvDUUv48by0Q68N016hTuaawb4orE5FUUkgIlVU13Pb4Ip55ZwMQ68P0u6uH8flTj01xZSKSagqJDLenMtaH6bX3Y32YcrKzmDy6gE8PUh8mEVFIZLTteyoZM62It9fG+jB1yWnDtDGFDOujPkwiEqOQyFAbd1Zw/ZT5rNy4C4BjOrVjxrhCBvXqmOLKRKQ5UUhkoNVl5Vw3ZR6l22J9mI7vkcuMcYXkd81JcWUi0twoJDLMsvU7uGFqEWW79wHwieM6MX1sIT3Uh0lEGqCQyCDzV21l3LQidgV9mAqP78aDNxTQqV2bFFcmIs1V5D0WzGykma00s2Izu7OB7XeY2aLg31IzqzazbsG21Wa2JNi2IOpaW7KX3t3I6Cnz6gLicyf34uGxhQoIETmkSEcSZpYFPABcCJQCRWY2292X1+7j7vcA9wT7Xwp80923xr3Mee5eFmWdLd2st0u5/YmP+zBdfkZvfvWl09SHSUQOK+rfEoVAsbuXuHslMBO47BD7XwM8GnFNGeWhN1bxzccW1wXE2LOP59dXDFVAiEhCov5N0RtYF7dcGqw7iJnlACOBv8StduAFM1toZhMiq7IFcnd+8+J7/PjpukEbt190Iv/1byfTqpUa9YlIYqJ+47qh30Yesu+lwBv1pprOdvf1ZtYTeNHMVrj7qwd8gVh4TADo21d9hgBqapwfPb2Mh+euAcAMfnrZKVz3yX4prkxE0k3UI4lSoE/ccj6wPmTfq6k31eTu64PHTcAsYtNX1NtnsrsXuHtBXl5eUopOZ5VVNXzjsUV1AdEmy7jv6tMVECLSJFGHRBEwyMyON7NsYkEwu/5OZtYZOAd4Km5drpl1rH0OXAQsjbjetLa3spoJMxYwe3Esh9u3yeLBG4Zz6dDjUlyZiKSrSKeb3L3KzG4BngeygKnuvszMJgbbJwW7jgJecPfyuMN7AbOCG920Bh5x9+eirDed7dizn7HTi1i4ZhsAndu3YeqNwzmzX9cUVyYi6Szyi+ncfQ4wp966SfWWpwHT6q0rAYZGXF6LsGlnBddPnc+Kj2J9mHp2bMuMcSM46Rj1YRKRI6MrrtPc2i17uG7KPNZu3QNAv+45/GncCPp0Ux8mETlyCok09u6GnVw/dT6bd8X6MJ18bCemjx1Oz47tUlyZiLQUCok0tWD1VsZOK2JnRazNxvD+XXnwhuF0bq82GyKSPAqJZq6quoY5Sz9i484KALbs3scj89bwk2eWU7G/BoDzB/fkgWvPoH12VipLFZEWSCHRjO2trGbstCLmlmypW7ezoorvzfr4k8BfHHYc93x5KG3UZkNEIqCQaMZ+94/3DgiI+q44sze/+tJQtdkQkcjoz89man91DTOL1h1yn2M7tVNAiEikFBLN1NbySnbs3X/IfUrK9hylakQkUykkmqmO7VrT+jCjhG652UepGhHJVAqJZionuzUjTznmkPuMOqPBrusiIkmjkGjGvjNyMHkd2za47doRfTmjr/oyiUi0FBLNWJ9uOfzt5rO5qqBP3Y052mQZd406hbu+eEpKaxORzKCQaOZ6d2nPL684jX7dY72Y8rvm8JUR/Qi644qIREohkSYUCiKSCgoJEREJpZAQEZFQCgkREQmlkBARkVAKCRERCaWQEBGRUAoJEREJpZAQEZFQCgkREQmlkBARkVAKCRERCRV5SJjZSDNbaWbFZnZnA9vvMLNFwb+lZlZtZt0SOVZERKIVaUiYWRbwAHAJMAS4xsyGxO/j7ve4+zB3HwZ8F3jF3bcmcqyIiEQr6pFEIVDs7iXuXgnMBC47xP7XAI828VgREUmyqEOiN7Aubrk0WHcQM8sBRgJ/aeyxIiISjahDoqGbIHjIvpcCb7j71sYca2YTzGyBmS3YvHlzE8sUEZGGRB0SpUCfuOV8YH3Ivlfz8VRTwse6+2R3L3D3gry8vCMsV0RE4kUdEkXAIDM73syyiQXB7Po7mVln4BzgqcYeKyIi0Wkd5Yu7e5WZ3QI8D2QBU919mZlNDLZPCnYdBbzg7uWHOzbKekVE5ECRhgSAu88B5tRbN6ne8jRgWiLHiojI0aMrrkVEJJRCQkREQikkREQklEJCRERCKSRERCSUQkJEREIpJEREJJRCQkREQikkREQklEJCRERCKSRERCSUQkJEREIpJEREJJRCQkREQikkREQklEJCRERCKSRERCSUQkJEREJFfvtSSY78ru0PeBQRORoUEmlixrgRqS5BRDKQpptERCSUQkJEREIpJEREJJRCQkREQikkREQklEJCRERCmbunuoakMbPNwJpU1xGhHkBZqouQJtP5S18t/dz1c/e8hja0qJBo6cxsgbsXpLoOaRqdv/SVyedO000iIhJKISEiIqEUEullcqoLkCOi85e+Mvbc6T0JEREJpZGEiIiEUkiIiEgohYSIiIRSSBxFZtbFzP49ya95l5mtM7Pd9da3NbPHzKzYzOaZWf9kft1Mk+xzZ2Y5Zvasma0ws2VmdnfcNp27JIvoZ+85M1scnL9JZpYVrG9R508hcXR1AZL6PyrwNFDYwPpxwDZ3PwH4LfDLJH/dTBPFufu1uw8GTgfONrNLgvU6d8kXxfm70t2HAqcAecCXg/Ut6vwpJI6uu4GBZrbIzO4J/i01syVmdhWAmZ1rZq+a2SwzWx78hRJ6ntz9TXff0MCmy4DpwfMngQvMzJL+HWWOpJ47d9/j7v8MnlcCbwH5wWadu+SL4mdvZ/C0NZAN1H5UtEWdP4XE0XUn8IG7DwPeBIYBQ4HPAfeY2bHBfoXAt4BTgYHA5U34Wr2BdQDuXgXsALofUfWZLbJzZ2ZdgEuBl4JVOnfJF8n5M7PngU3ALmKBAC3s/CkkUufTwKPuXu3uG4FXgOHBtvnuXuLu1cCjwb6N1dBfLrooJjmSdu7MrHWw333uXlK7uoFdde6SJ2nnz90vBo4F2gLnB6tb1PlTSKTOoYaf9f+Hasr/YKVAH6j7RdQZ2NqE15GDJfPcTQbed/ffxa3TuYtWUn/23L0CmE1smgla2PlTSBxdu4COwfNXgavMLMvM8oDPAvODbYVmdnwwH3oV8HoTvtZs4Ibg+RXAy67L649E0s+dmf2M2C+Qb9TbpHOXfEk9f2bWoXaKKgiCzwMrgs0t6vwpJI4id98CvGFmS4GzgHeAxcDLwLfd/aNg17nE3mhbCqwCZoW9ppn9ysxKgRwzKzWzHwWbpgDdzawYuI3YnKw0UbLPnZnlA98HhgBvBW+ojg8269wlWQQ/e7nAbDOrfZ1NwKRgW4s6f+rd1MyY2bnA7e7+b6muRRpH5y696fw1TCMJEREJpZFEmjCzecQ+QRFvtLsvSUU9kjidu/SW6edPISEiIqE03SQiIqEUEiL1RNEMTiRdKSREDhZFMziRtKSQEDlY0pvBmdlui7V1X2xmb5pZr2D9NDO7In6/uNd/xcweN7P3zOxuM/uKmc0P6hgY8X8DEUAhIdKQKJrB5QJvBq2lXwW+mkAdQ4Fbg9cfDZzo7oXAg8DXG/tNiTSFQkLk0JLVDK4SeCZ4vhDon8DXLnL3De6+D/gAeCFYvyTB40WOmEJC5NCS1Qxuf1z/nmpi9yAAqCL4OQzuOUYpGwUAAACESURBVJAdd8y+uOc1ccs1cceLREohIXKwo9mIcTVwZvD8MqBNk6sWiYBCQqSeKBoxHsIfgXPMbD4wAig/wvJFkkpXXIs0gZrBSabQSEJEREJpJCGSRJneDE5aHoWEiIiE0nSTiIiEUkiIiEgohYSIiIRSSIiISCiFhIiIhFJIiIhIqP8DeCy44eLu89sAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# write a function to extract left-out subject id and the classification acc. \n",
    "def extract_clf_acc(base_dir, top_num, num_subjs):\n",
    "    \n",
    "    import os\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    path = os.path.join(base_dir, top_num, 'classify_result.txt')\n",
    "\n",
    "    # read each line, from which extract the left out subject id and the clf acc\n",
    "    with open(path, \"r\") as file:\n",
    "        lines = file.readlines()\n",
    "    loo_id = []\n",
    "    acc = []\n",
    "    for line in lines: \n",
    "        id_start_index = line.index('_no') + len('_no')\n",
    "        id_end_index = line.index('_result_seq')\n",
    "        id = int(line[id_start_index:id_end_index])\n",
    "        loo_id.append(id)\n",
    "\n",
    "        acc_start_index = line.index(': ') + 2\n",
    "        acc_end_index = line.index('\\n')\n",
    "        score = float(line[acc_start_index:acc_end_index])\n",
    "        acc.append(score)\n",
    "\n",
    "    # write a dataframe\n",
    "    \n",
    "    colnames = ['loo_id','clf_acc','top_num']\n",
    "    df = pd.DataFrame(index=range(len(loo_id)), columns=colnames)\n",
    "    df['loo_id'] = loo_id \n",
    "    df['clf_acc'] = acc\n",
    "    df['top_num'] = np.repeat(top_num, len(loo_id)).tolist()\n",
    "        \n",
    "    return df\n",
    "\n",
    "# generate df and concatenate them together\n",
    "df_list = [extract_clf_acc(output_dir,'top_10', num_subjs),\n",
    "           extract_clf_acc(output_dir,'top_20', num_subjs),\n",
    "           extract_clf_acc(output_dir,'top_30', num_subjs)]\n",
    "final_df = pd.concat(df_list).reset_index(drop=True)\n",
    "\n",
    "# a quick look at the df \n",
    "ax = sns.pointplot(x=\"top_num\", y=\"clf_acc\", data=final_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Take Aways\n",
    "- Tie back our two initial questions: we asked \n",
    "    1. Do functional connectivity patterns differ across condition A and B \n",
    "    2. What are the brain regions among which the FC pattern characterize each task condition? \n",
    "- Answering Q1: \n",
    "    - Based on the scatter plot above, we can argue the functional connectivity pattern of the top n performed features can be used to differentiate whether a subject is doing condition A or B. Thus, the answer to question 1 is yes, the FC patterns do differ across condition A and B.\n",
    "- Answering Q2:\n",
    "    - Let's take n = 30 as an example. Remember that the top 30 features for each training sets may be different given that we have 8 trainig sets (one for each left out subject). Thus a given voxel may appear 0 - 8 times in these 8 top-30-feature masks, resulting a stats map. (i.e., 'prop_top30.nii.gz')\n",
    "    - Let's look at out cherry picked brain again, which is consist of 400 voxels in total. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 400 voxel brain\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<nilearn.plotting.displays.OrthoSlicer at 0x7fec94884dd8>"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 475.2x187.2 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"The 400 voxel brain\")\n",
    "plotting.plot_epi(image.load_img(os.path.join(cur_dir,'cherry_picked_brain','cherry_pick_brain_mask.nii.gz')),(65,43,80))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Then we overlap the top-30 feature stats map onto it and observe where they locate. Note that these features may not be meaningful in this simulated dataset, but will be in a real data set. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 475.2x187.2 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display = plotting.plot_epi(image.load_img(os.path.join(cur_dir,'cherry_picked_brain','cherry_pick_brain_mask.nii.gz')),cut_coords = (65,43,80))\n",
    "display.add_overlay(image.load_img(os.path.join(output_dir, 'top_30','prop_top30.nii.gz')), cmap=plotting.cm.black_pink)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
